{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -*- Nuha Alghamdi -*-\n",
    "### -*- nuhaalghamdi92@gmail.com -*-\n",
    "### -*- Feb 22 2020-*-\n",
    "\n",
    "* Here we used the official library by Facebook\n",
    "* You can download Facebook official fastText library from [here](https://pypi.org/project/fasttext/)\n",
    "* You can download Gensim fastText library from [here](https://pypi.org/project/gensim/)\n",
    "* Special thanks to Aziz Altowayan for using some functions from [his repository](https://github.com/iamaziz/ar-embeddings/blob/master/asa.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facebook fastText library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import fastText\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load wiki.ar model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Write your own model path\n",
    "mpath = 'D:\\\\dataset\\\\wiki.ar.bin'\n",
    "m = fastText.load_model(mpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to generate vectors for text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vw(w):\n",
    "    \"\"\"return word vector\"\"\"\n",
    "    return m.get_word_vector(w)\n",
    "\n",
    "def vs(s):\n",
    "    \"\"\"return sentence vector\"\"\"\n",
    "    return m.get_sentence_vector(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Put data in dataframe\n",
    "cols = ['data','label']\n",
    "#write your own dataset path\n",
    "df=pd.read_csv('D:\\\\dataset\\\\dataset16Feb.csv', names=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Shuffle data\n",
    "df=df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Prepare variables\n",
    "dimension = 300 # vector dimension\n",
    "len_examples = df.shape[0]  #no. of articles in the dataset\n",
    "\n",
    "# all vectors initialized with zeros\n",
    "vecs = np.zeros((len_examples, dimension), dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get sentence vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(len_examples):\n",
    "    try:\n",
    "        vecs[i] = vs(df['data'].iloc[i])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#To make labels as digits from 0 to 4 (five labels)\n",
    "digit_to_label=dict(enumerate(set(df['label']))) \n",
    "label_to_digit={v:k for k,v in digit_to_label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X refers to vectors and y is their labels\n",
    "X= vecs\n",
    "y=np.array(df['label'].apply(lambda l:label_to_digit[l] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the sentence vectors and their labels in your folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_vecs_fastText_vs is the name of the file that contains the generated sentence vectors by official fastText\n",
    "#y_labels_fastText_vs is the name of the file that contains their labels\n",
    "np.save('X_vecs_fastText_vs', X)\n",
    "np.save('y_labels_fastText_vs', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Average word vectors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import nltk\n",
    "tokenized_data=[]\n",
    "tokenized_words=[]\n",
    "for i in range(len_examples):\n",
    "    tokenized_words = nltk.word_tokenize(df['data'].iloc[i])\n",
    "    tokenized_data.append(tokenized_words)\n",
    "    tokenized_words=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Just to make sure of the number of articles (1909 articles)\n",
    "#print(len(tokenized_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next four cells are taking the tokenized data and the fastText model to generate vector for each word then average the vectors of the same article to get one vector for each article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from logging import info, INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature(words,pretrainedmodel):\n",
    "    \"\"\"average words' vectors\"\"\"\n",
    "    dimension=300\n",
    "    embeddings=pretrainedmodel\n",
    "    feature_vec = np.zeros((dimension,), dtype=\"float32\")\n",
    "    retrieved_words = 0\n",
    "    for token in words:\n",
    "        try:\n",
    "            feature_vec = np.add(feature_vec,embeddings.get_word_vector(token))\n",
    "            retrieved_words += 1\n",
    "        except KeyError:\n",
    "            pass  # if a word is not in the embeddings' vocabulary discard it\n",
    "\n",
    "    np.seterr(divide='ignore', invalid='ignore')\n",
    "    feature_vec = np.divide(feature_vec, retrieved_words)\n",
    "\n",
    "    return feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def average_feature_vectors(examples,pretrainedmodel, type_='NaN'):\n",
    "    \"\"\"\n",
    "    :param examples: a list of lists (each list contains words) e.g. [['hi','do'], ['you','see'], ... ]\n",
    "    :param type_: (optional) type of examples text e.g. train / test\n",
    "    :return: the average word vector of each list\n",
    "    \"\"\"\n",
    "    dimension=300\n",
    "    embeddings=pretrainedmodel\n",
    "    feature_vectors = np.zeros((len(examples), dimension), dtype=\"float32\")\n",
    "    info(\"Vectorizing {} tokens ..\".format(type_))\n",
    "    for i, example in enumerate(examples):\n",
    "        feature_vectors[i] = feature(example,embeddings)\n",
    "\n",
    "    info(\" ... total {} {}\".format(len(feature_vectors), type_))\n",
    "\n",
    "    return feature_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "avg_vw=average_feature_vectors(tokenized_data,m) #average word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the average word vectors and their labels in your folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=avg_vw\n",
    "np.save('X_vecs_fastText_vw', X)\n",
    "np.save('y_labels_fastText_vw', y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
