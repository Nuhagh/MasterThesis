{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -*- Nuha Alghamdi -*-\n",
    "### -*- nuhaalghamdi92@gmail.com -*-\n",
    "### -*- Feb 22 2020-*-\n",
    "\n",
    "* Here we use Gensim fastText.\n",
    "* You can download the library from [here](https://pypi.org/project/gensim/)\n",
    "* Special thanks to Aziz Altowayan for using some functions from [his repository](https://github.com/iamaziz/ar-embeddings/blob/master/asa.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim fastText library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\Anaconda3\\envs\\tf18\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.fasttext import FastText as FT_gensim\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load wiki.ar model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mpath = 'D:\\\\dataset\\\\wiki.ar.bin'\n",
    "m1= FT_gensim.load_fasttext_format(mpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Put data in dataframe\n",
    "cols = ['data','label']\n",
    "#write your own dataset path\n",
    "df=pd.read_csv('D:\\\\dataset\\\\dataset16Feb.csv', names=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Shuffle data\n",
    "df=df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get sentence vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to get vector for an article (as one sentence)\n",
    "def get_vector(txt):\n",
    "    vec=m1[txt]\n",
    "    \n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Prepare variables to put vectors in\n",
    "dimension = 300 # vector dimension\n",
    "len_examples = df.shape[0] #no. of examples\n",
    "\n",
    "# all vectors\n",
    "vecs = np.zeros((len_examples, dimension), dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(len_examples):\n",
    "    try:\n",
    "        vecs[i] = get_vector(df['data'].iloc[i])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to make the labels in numbers from 0 to 4 instead of text\n",
    "digit_to_label=dict(enumerate(set(df['label'])))\n",
    "label_to_digit={v:k for k,v in digit_to_label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X= vecs\n",
    "y=np.array(df['label'].apply(lambda l:label_to_digit[l] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the sentence vectors and their labels in your folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_vecs_gen is the file name contains sentence vectors generated by gensim fastText\n",
    "#y_labels_gen is the file name contains their labels\n",
    "np.save('X_vecs_gen', X) \n",
    "np.save('y_labels_gen', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Average word vectors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next 5 cells are functions to tokenize the articles then get vector for each word in each articlee then average the vectors of each article to get one vector representing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from logging import info, INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature(words,pretrainedmodel):\n",
    "    \"\"\"average words' vectors\"\"\"\n",
    "    dimension=300\n",
    "    embeddings=pretrainedmodel\n",
    "    feature_vec = np.zeros((dimension,), dtype=\"float32\")\n",
    "    retrieved_words = 0\n",
    "    for token in words:\n",
    "        try:\n",
    "            feature_vec = np.add(feature_vec,embeddings[token])\n",
    "            retrieved_words += 1\n",
    "        except KeyError:\n",
    "            pass  # if a word is not in the embeddings' vocabulary discard it\n",
    "\n",
    "    np.seterr(divide='ignore', invalid='ignore')\n",
    "    feature_vec = np.divide(feature_vec, retrieved_words)\n",
    "\n",
    "    return feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def average_feature_vectors(examples,pretrainedmodel, type_='NaN'):\n",
    "    \"\"\"\n",
    "    :param examples: a list of lists (each list contains words) e.g. [['hi','do'], ['you','see'], ... ]\n",
    "    :param type_: (optional) type of examples text e.g. train / test\n",
    "    :return: the average word vector of each list\n",
    "    \"\"\"\n",
    "    dimension=300\n",
    "    embeddings=pretrainedmodel\n",
    "    feature_vectors = np.zeros((len(examples), dimension), dtype=\"float32\")\n",
    "    info(\"Vectorizing {} tokens ..\".format(type_))\n",
    "    for i, example in enumerate(examples):\n",
    "        feature_vectors[i] = feature(example,embeddings)\n",
    "\n",
    "    info(\" ... total {} {}\".format(len(feature_vectors), type_))\n",
    "\n",
    "    return feature_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import nltk\n",
    "tokenized_data=[]\n",
    "tokenized_words=[]\n",
    "for i in range(len_examples):\n",
    "    tokenized_words = nltk.word_tokenize(df['data'].iloc[i])\n",
    "    tokenized_data.append(tokenized_words)\n",
    "    tokenized_words=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "avg_vw=average_feature_vectors(tokenized_data,m1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the average word vectors and their labels in your folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=avg_vw\n",
    "np.save('X_vecs_tokenized_gen', X)\n",
    "np.save('y_labels_tokenized_gen', y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
